{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "from sklearn import set_config\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector, make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides notes for common classes and methods used in scikit-learn. It also answers questions to concepts I had trouble understanding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(120, 4)\n"
     ]
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True, as_frame=True)\n",
    "rng = np.random.RandomState(0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rng)\n",
    "print(X.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** An estimator is any object in scikit-learn that learns from data by calling `fit()`.\n",
    "\n",
    "*   It can be a transformer (e.g., `StandardScaler`), a predictor (e.g., `LogisticRegression`), or both.\n",
    "*   Essentially, if an object has a `fit()` method, it's an estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-15 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-15 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-15 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-15 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-15 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-15 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-15 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-15 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-15 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-15 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-15 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-15 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-15 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-15 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-15 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-15 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-15 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-15 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-15 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-15\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" checked><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()  # Estimator\n",
    "model = LogisticRegression()  # Estimator\n",
    "\n",
    "scaler.fit(X_train)  # ✅ Works because StandardScaler is an estimator\n",
    "model.fit(X_train, y_train)  # ✅ Works because LogisticRegression is an estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** A transformer is a special type of estimator that modifies data by calling `transform()`.\n",
    "\n",
    "*   Transformers are used for preprocessing (scaling, encoding, feature selection, etc.).\n",
    "*   They learn something from `fit()` and apply it with `transform()`.\n",
    "*   Many transformers also support `fit_transform()` to do both in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "encoder = OneHotEncoder()\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "scaler.fit(X_train)  # ✅ Learns mean & std\n",
    "X_scaled = scaler.transform(X_train)  # ✅ Applies scaling\n",
    "\n",
    "X_scaled = scaler.fit_transform(X_train)  # ✅ Shortcut (fit + transform)\n",
    "\n",
    "pca.fit(X_train)  # ✅ Learns principal components\n",
    "X_pca = pca.transform(X_train)  # ✅ Applies dimensionality reduction\n",
    "X_pca = pca.fit_transform(X_train)  # ✅ Shortcut (fit + transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rule:** If an object has both `fit()` and `transform()`, it's a transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** A predictor is an estimator that makes predictions by calling `predict()`.\n",
    "\n",
    "*   Predictors are models that take input data and output predictions.\n",
    "*   Some predictors also support `predict_proba()` for probability estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()  # Predictor\n",
    "rf = RandomForestClassifier()  # Predictor\n",
    "\n",
    "clf.fit(X_train, y_train)  # ✅ Learns from data\n",
    "y_pred = clf.predict(X_test)  # ✅ Makes predictions\n",
    "\n",
    "rf.fit(X_train, y_train)  # ✅ Learns from data\n",
    "y_prob = rf.predict_proba(X_test)  # ✅ Outputs class probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rule:** If an object has `predict()`, it's a predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (30, 4)\n",
      "(120,) (30,)\n",
      "False\n",
      "\n",
      "Best Estimator:\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('model',\n",
      "                 SGDClassifier(alpha=7e-05, penalty='l1',\n",
      "                               random_state=RandomState(MT19937) at 0x213E83E7B40))])\n",
      "Best Params:\n",
      "{'model__alpha': 7e-05, 'model__penalty': 'l1'} \n",
      "Best score:\n",
      "0.95\n",
      "\n",
      "CV Results\n",
      "\n",
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0        0.006735      0.001569         0.002067        0.000574   \n",
      "1        0.005635      0.000948         0.001767        0.000559   \n",
      "2        0.005235      0.000844         0.001834        0.000454   \n",
      "3        0.003401      0.000490         0.001234        0.000423   \n",
      "4        0.004201      0.000600         0.001434        0.000496   \n",
      "5        0.003868      0.000670         0.001300        0.000458   \n",
      "6        0.003634      0.000547         0.001434        0.000495   \n",
      "7        0.003368      0.000706         0.001167        0.000373   \n",
      "8        0.003634      0.000706         0.001300        0.000458   \n",
      "9        0.003601      0.000611         0.001300        0.000458   \n",
      "10       0.003501      0.000563         0.001267        0.000442   \n",
      "11       0.003468      0.000499         0.001167        0.000373   \n",
      "\n",
      "    param_model__alpha param_model__penalty  \\\n",
      "0              0.00010                   l2   \n",
      "1              0.00010                   l1   \n",
      "2              0.00010           elasticnet   \n",
      "3              0.00003                   l2   \n",
      "4              0.00003                   l1   \n",
      "5              0.00003           elasticnet   \n",
      "6              0.00005                   l2   \n",
      "7              0.00005                   l1   \n",
      "8              0.00005           elasticnet   \n",
      "9              0.00007                   l2   \n",
      "10             0.00007                   l1   \n",
      "11             0.00007           elasticnet   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0    {'model__alpha': 0.0001, 'model__penalty': 'l2'}               0.75   \n",
      "1    {'model__alpha': 0.0001, 'model__penalty': 'l1'}               1.00   \n",
      "2   {'model__alpha': 0.0001, 'model__penalty': 'el...               1.00   \n",
      "3     {'model__alpha': 3e-05, 'model__penalty': 'l2'}               1.00   \n",
      "4     {'model__alpha': 3e-05, 'model__penalty': 'l1'}               0.75   \n",
      "5   {'model__alpha': 3e-05, 'model__penalty': 'ela...               1.00   \n",
      "6     {'model__alpha': 5e-05, 'model__penalty': 'l2'}               1.00   \n",
      "7     {'model__alpha': 5e-05, 'model__penalty': 'l1'}               1.00   \n",
      "8   {'model__alpha': 5e-05, 'model__penalty': 'ela...               1.00   \n",
      "9     {'model__alpha': 7e-05, 'model__penalty': 'l2'}               1.00   \n",
      "10    {'model__alpha': 7e-05, 'model__penalty': 'l1'}               1.00   \n",
      "11  {'model__alpha': 7e-05, 'model__penalty': 'ela...               1.00   \n",
      "\n",
      "    split1_test_score  split2_test_score  ...  split23_test_score  \\\n",
      "0                 1.0               0.75  ...                1.00   \n",
      "1                 1.0               0.75  ...                0.75   \n",
      "2                 1.0               0.75  ...                0.75   \n",
      "3                 1.0               0.75  ...                0.75   \n",
      "4                 1.0               0.75  ...                0.75   \n",
      "5                 1.0               1.00  ...                0.75   \n",
      "6                 1.0               0.75  ...                1.00   \n",
      "7                 1.0               0.75  ...                0.75   \n",
      "8                 1.0               0.75  ...                0.75   \n",
      "9                 1.0               1.00  ...                1.00   \n",
      "10                1.0               1.00  ...                0.75   \n",
      "11                1.0               0.75  ...                1.00   \n",
      "\n",
      "    split24_test_score  split25_test_score  split26_test_score  \\\n",
      "0                  1.0                 1.0                 1.0   \n",
      "1                  1.0                 1.0                 1.0   \n",
      "2                  1.0                 1.0                 1.0   \n",
      "3                  1.0                 1.0                 1.0   \n",
      "4                  1.0                 1.0                 1.0   \n",
      "5                  1.0                 1.0                 1.0   \n",
      "6                  1.0                 1.0                 1.0   \n",
      "7                  1.0                 1.0                 1.0   \n",
      "8                  1.0                 1.0                 1.0   \n",
      "9                  1.0                 1.0                 1.0   \n",
      "10                 1.0                 1.0                 1.0   \n",
      "11                 1.0                 1.0                 1.0   \n",
      "\n",
      "    split27_test_score  split28_test_score  split29_test_score  \\\n",
      "0                 0.75                 1.0                 1.0   \n",
      "1                 0.75                 1.0                 1.0   \n",
      "2                 0.75                 1.0                 1.0   \n",
      "3                 0.75                 1.0                 1.0   \n",
      "4                 0.75                 1.0                 1.0   \n",
      "5                 0.75                 1.0                 1.0   \n",
      "6                 0.75                 1.0                 1.0   \n",
      "7                 0.75                 1.0                 1.0   \n",
      "8                 0.75                 1.0                 1.0   \n",
      "9                 0.75                 1.0                 1.0   \n",
      "10                0.75                 1.0                 1.0   \n",
      "11                0.75                 1.0                 1.0   \n",
      "\n",
      "    mean_test_score  std_test_score  rank_test_score  \n",
      "0          0.900000        0.138444                8  \n",
      "1          0.933333        0.110554                4  \n",
      "2          0.916667        0.134371                6  \n",
      "3          0.891667        0.139692               10  \n",
      "4          0.883333        0.167498               11  \n",
      "5          0.883333        0.140436               11  \n",
      "6          0.900000        0.152753                8  \n",
      "7          0.933333        0.110554                4  \n",
      "8          0.908333        0.120474                7  \n",
      "9          0.941667        0.105738                2  \n",
      "10         0.950000        0.100000                1  \n",
      "11         0.941667        0.123884                2  \n",
      "\n",
      "[12 rows x 40 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>target_digit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1797 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1     2     3     4     5    6    7    8    9  ...   55   56  \\\n",
       "0     0.0  0.0   5.0  13.0   9.0   1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1     0.0  0.0   0.0  12.0  13.0   5.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "2     0.0  0.0   0.0   4.0  15.0  12.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "3     0.0  0.0   7.0  15.0  13.0   1.0  0.0  0.0  0.0  8.0  ...  0.0  0.0   \n",
       "4     0.0  0.0   0.0   1.0  11.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "...   ...  ...   ...   ...   ...   ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "1792  0.0  0.0   4.0  10.0  13.0   6.0  0.0  0.0  0.0  1.0  ...  0.0  0.0   \n",
       "1793  0.0  0.0   6.0  16.0  13.0  11.0  1.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1794  0.0  0.0   1.0  11.0  15.0   1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1795  0.0  0.0   2.0  10.0   7.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1796  0.0  0.0  10.0  14.0   8.0   1.0  0.0  0.0  0.0  2.0  ...  0.0  0.0   \n",
       "\n",
       "       57   58    59    60    61   62   63  target_digit  \n",
       "0     0.0  6.0  13.0  10.0   0.0  0.0  0.0             0  \n",
       "1     0.0  0.0  11.0  16.0  10.0  0.0  0.0             1  \n",
       "2     0.0  0.0   3.0  11.0  16.0  9.0  0.0             2  \n",
       "3     0.0  7.0  13.0  13.0   9.0  0.0  0.0             3  \n",
       "4     0.0  0.0   2.0  16.0   4.0  0.0  0.0             4  \n",
       "...   ...  ...   ...   ...   ...  ...  ...           ...  \n",
       "1792  0.0  2.0  14.0  15.0   9.0  0.0  0.0             9  \n",
       "1793  0.0  6.0  16.0  14.0   6.0  0.0  0.0             0  \n",
       "1794  0.0  2.0   9.0  13.0   6.0  0.0  0.0             8  \n",
       "1795  0.0  5.0  12.0  16.0  12.0  0.0  0.0             9  \n",
       "1796  1.0  8.0  12.0  14.0  12.0  1.0  0.0             8  \n",
       "\n",
       "[1797 rows x 65 columns]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X, y = load_digits(return_X_y=True)\n",
    "# df = pd.DataFrame(X)\n",
    "# df['target_digit'] = y\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=rng)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "duplicates = [np.any(np.all(row == X_train, axis=1)) for row in X_test]\n",
    "print(np.any(duplicates))  # Should be False\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', SGDClassifier(random_state=rng))\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {'model__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'model__alpha': [0.0001, 0.00003, 0.00005, 0.00007]}\n",
    "]\n",
    "\n",
    "# pipe.fit(X_train, y_train)\n",
    "# # training set\n",
    "# y_pred_train = pipe.predict(X_train)\n",
    "# y_pred_train_score = classification_report(y_train, y_pred_train) \n",
    "# # test set\n",
    "# y_pred = pipe.predict(X_test)\n",
    "# y_score = classification_report(y_test, y_pred)\n",
    "# print(f\"Score on train set:\\n{y_pred_train_score}\\n\\nScore on test data\\n{y_score}\")\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=30, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest Estimator:\\n{grid.best_estimator_}\\nBest Params:\\n{grid.best_params_} \\nBest score:\\n{grid.best_score_}\\n\\nCV Results\\n\\n{pd.DataFrame(grid.cv_results_)}\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       0.93      1.00      0.96        13\n",
      "           2       1.00      0.83      0.91         6\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.98      0.94      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = grid.best_estimator_.predict(X_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.875      0.95833333 0.91666667 1.         0.79166667]\n",
      "\n",
      "Avg accuracy: \n",
      " 0.9083333333333334\n"
     ]
    }
   ],
   "source": [
    "cv_score = cross_val_score(grid.best_estimator_, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(cv_score)\n",
    "print('\\nAvg accuracy: \\n', cv_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Order\n",
    "**Question**: So when predictions = `pipe.predict(X_test)` is called, is it taking the scaler into account?\n",
    "- Yes! ✅ When you call: predictions = pipe.predict(X_test), the entire pipeline is applied, meaning both the scaler and the classifier are used.\n",
    "\n",
    "When you call `pipe.predict(X_test)`, the pipeline follows these steps in order:\n",
    "\n",
    "1.  **`StandardScaler` transforms `X_test`:**\n",
    "    The raw test data (`X_test`) is scaled using the `MinMaxScaler` that was already fitted on `X_train`.\n",
    "\n",
    "2.  **`SGDClassifier` makes predictions on the scaled data:**\n",
    "    The classifier (`SGDClassifier`) receives the transformed data and makes predictions.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Param Grid Syntax\n",
    "syntax is :\n",
    "\n",
    "*estimator*__*parameter* (estimator, two underscores, parameter)\n",
    "\n",
    "**Question**: What is the difference (using above pipeline's example) between\n",
    "\n",
    "```\n",
    "param_grid = [\n",
    "    {'model__penalty': ['l2', 'l1', 'elasticnet']},\n",
    "    {'model__alpha': [0.0001, 0.001, 0.01, 0.1]}\n",
    "]\n",
    "```\n",
    "\n",
    "and\n",
    "\n",
    "```\n",
    "param_grid = {\n",
    "    'model__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'model__alpha': [0.0001, 0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this has TWO different grids (there is a list of dictionaries)\n",
    "param_grid = [\n",
    "    {'model__penalty': ['l2', 'l1', 'elasticnet']}, #grid 1\n",
    "    {'model__alpha': [0.0001, 0.001, 0.01, 0.1]} #grid 2\n",
    "]\n",
    "\n",
    "# this is ONE grid (one dictionary)\n",
    "param_grid = {\n",
    "    'model__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'model__alpha': [0.0001, 0.001, 0.01, 0.1]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **first** param_grid will **NOT** consider all combinations of `penalty` and `alpha`, `GridSearchCV` treats them as two separate independent grids.\n",
    "\n",
    "It will:\n",
    "\n",
    "- 1. Try tuning only `model__penalty` while keeping the default value for `model__alpha` (which is 0.0001).\n",
    "- 2. Then, it will try tuning only `model__alpha` while keeping the default value for `model__penalty` (which is 'l2').\n",
    "\n",
    "The **second** param_grid will **WILL** consider all combinations of `penalty` and `alpha`, `GridSearchCV` treats them as one grid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replaces NaNs with a strategy like mean, median, mode, or a constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 2. , 7.5],\n",
       "       [4. , 5. , 6. ],\n",
       "       [7. , 8. , 9. ]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "X_2 = np.array([[1, 2, np.nan], [4, np.nan, 6], [7, 8, 9]])\n",
    "imputer = SimpleImputer(strategy=\"mean\")  # Replace NaNs with column mean\n",
    "X_imputed = imputer.fit_transform(X_2)\n",
    "X_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IterativeImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimates missing values by predicting them based on other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 2.        , 3.00203274],\n",
       "       [4.        , 4.99796925, 6.        ],\n",
       "       [7.        , 8.        , 9.        ]])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "imputer = IterativeImputer()\n",
    "X_imputed = imputer.fit_transform(X_2)\n",
    "X_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *VarianceThreshold* in scikit-learn is a simple baseline *filter method* for feature selection. It removes features that have low variance — meaning features that don't change much across samples.\n",
    "- If a feature has the same value (or nearly the same value) for all samples, it doesn’t provide useful information to the model.\n",
    "Such features don’t help in distinguishing between classes or predicting targets, so removing them can simplify the model without losing performance.\n",
    "\n",
    "- Such features don’t help in distinguishing between classes or predicting targets, so removing them can simplify the model without losing performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68112222 0.18871289 3.09550267 0.57713289]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Initialize with a threshold — features with variance below this will be removed\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "\n",
    "# Fit and transform the data\n",
    "X_filtered = selector.fit_transform(X)\n",
    "\n",
    "# To check variances of all features\n",
    "print(selector.variances_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Threshold* = 0 (default): Removes features with zero variance (constant features).\n",
    "- Higher thresholds: Remove features with variance lower than the given value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn’s `mutual_info_classif` and `mutual_info_regression` are *filter methods* and measure how much information each feature contributes to predicting the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Feature  MI Score\n",
      "3   petal width (cm)  0.996625\n",
      "2  petal length (cm)  0.988786\n",
      "0  sepal length (cm)  0.491034\n",
      "1   sepal width (cm)  0.271356\n",
      "Features selected by mutual information: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Calculate mutual information scores\n",
    "mi_scores = mutual_info_classif(X, y)\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "mi_df = pd.DataFrame({'Feature': X.columns, 'MI Score': mi_scores})\n",
    "print(mi_df.sort_values(by='MI Score', ascending=False))\n",
    "\n",
    "# Keep features above a threshold\n",
    "threshold = 0.05  # Adjust based on your dataset\n",
    "selected_features = X.columns[mi_scores > threshold]\n",
    "print(\"Features selected by mutual information:\", selected_features.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Square Test (chi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Purpose: Tests the independence of categorical variables by comparing observed and expected frequencies.\n",
    "- Use Case: When you have categorical features and want to test their dependence on the target (often used for classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.4, 0.2],\n",
       "       [1.4, 0.2],\n",
       "       [1.3, 0.2],\n",
       "       [1.5, 0.2],\n",
       "       [1.4, 0.2]])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "selector = SelectKBest(chi2, k=2)  # Select the top 5 features\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "X_selected[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA F-Test (f_classif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Purpose: Performs a statistical test to determine the relationship between each feature and the target variable (for classification tasks).\n",
    "- Use Case: When you want to assess the strength of the relationship between each feature and the target, particularly in classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.4, 0.2],\n",
       "       [1.4, 0.2],\n",
       "       [1.3, 0.2],\n",
       "       [1.5, 0.2],\n",
       "       [1.4, 0.2]])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "selector = SelectKBest(f_classif, k=2)  # Select the top 2 features based on F-test\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "X_selected[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Purpose: Selects features based on the importance values derived from a fitted model. This is model-dependent, so it works only after training a model that assigns feature importances (e.g., decision trees, Lasso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dpons\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "selector = SelectFromModel(model, threshold=\"mean\")  # Select features above mean importance\n",
    "X_selected = selector.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelectKBest (general Selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose: Selects the top k features based on a given scoring function (e.g., chi2, f_classif, etc.). It's a general-purpose feature selection method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VarianceThreshold: Removes low-variance features.\n",
    "- Mutual Information: Measures how much information a feature shares with the target.\n",
    "- Chi-Square Test: Evaluates dependence for categorical data.\n",
    "- ANOVA F-Test: Tests the relationship between features and the target for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Method**           | **Description**                                                    | **Use Case**                                                                                                                                                                |\n",
    "|----------------------|--------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **VarianceThreshold** | Removes features with low variance.                                | **Use when**: You want to remove features that don't vary much across the dataset, such as constant or near-constant features. Features with very low variance often don’t carry significant information, and removing them can reduce noise. For example, in a dataset where many rows have the same value for a feature (e.g., a feature indicating gender where 99% of entries are male), this feature won't help in prediction and can be discarded. |\n",
    "| **Mutual Information**| Measures how much information each feature shares with the target. | **Use when**: You **expect a non-linear relationship** between features and the target. Mutual information captures more complex, non-linear interactions that may not be apparent in linear models. For example, in tasks like image classification or predicting customer churn, there may be complex dependencies between features (e.g., user behavior patterns) and the target (e.g., whether the customer churns or not). Mutual information can uncover those patterns even when they are non-linear. |\n",
    "| **Chi-Square Test**   | Evaluates dependence for categorical data.                         | **Use when**: Your **data is categorical and you're working on a classification** task. This test is useful for determining if any of your categorical features are independent of the target variable. For example, in a dataset of customer survey responses where the target variable is \"Product Purchased\" (a categorical target), you can use the Chi-Square test to assess the dependence between each feature (e.g., age group, income level) and the target. It helps you focus on features that are more likely to influence the target. |\n",
    "| **ANOVA F-Test**      | Tests the relationship between features and the target variable (classification). | **Use when**: Your **target is categorical and your features are continuous**. The ANOVA F-test is used to determine whether the means of different groups (based on the target classes) are statistically different from each other. For example, in a dataset predicting income categories (low, medium, high), and using continuous features such as years of education and job experience, the ANOVA F-test helps identify which features vary significantly across the income categories. |\n",
    "| **SelectFromModel**   | Selects features based on model feature importance after fitting a model. | **Use when**: You have a trained model that computes feature importance (such as a decision tree, random forest, or Lasso regression). After fitting the model, SelectFromModel allows you to select the most important features as determined by the model’s internal mechanism. This method is particularly useful when you want feature selection to align directly with the model’s learning, improving efficiency by focusing on features most relevant to the target. For example, in a random forest model predicting loan default, features like \"credit score\" and \"loan amount\" might be identified as most important. |\n",
    "| **SelectKBest**       | Selects the top k features based on a scoring function (e.g., F-test, Chi-Square, etc.). | **Use when**: You want to select the top **k** features based on a specific scoring function. This is ideal when you have a large set of features and need a fixed number of the most relevant ones. It works well when you're aiming to reduce the dimensionality of the data while keeping the most informative features. For example, when you have a dataset with hundreds of features, SelectKBest can help you choose the 5 most important features based on their relationship with the target (e.g., using the F-test or Chi-Square test). |\n",
    "| **SelectPercentile**  | Selects a specified percentile of features based on a scoring function. | **Use when**: You want to select a specified proportion (percentile) of features instead of a fixed number. This method is useful when you don't want to predefine the exact number of features to keep, but rather the percentage of the best features based on their importance score. For example, in a dataset with thousands of features, you might want to keep the top 10% of features, which are the most strongly correlated with the target variable. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maybe Best Way - Pandas Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.corr() is a *filter method*. If two features have a correlation above a threshold (e.g., 0.9), drop one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150, 3)\n"
     ]
    }
   ],
   "source": [
    "corr_matrix = X.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with high correlation\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n",
    "\n",
    "# Drop highly correlated features\n",
    "df_reduced = X.drop(to_drop, axis=1)\n",
    "print(X.shape)\n",
    "print(df_reduced.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) reduces redundancy by transforming correlated features into a set of uncorrelated components.\n",
    "\n",
    "\n",
    "**1. To Reduce the Number of Features (Dimensionality Reduction)**\n",
    "\n",
    "*   If your dataset has many features (high dimensionality), PCA can help reduce the number while keeping most of the information.\n",
    "*   This is useful because high-dimensional data can lead to the curse of dimensionality, making models slower and more prone to overfitting.\n",
    "*   **📌 Use case:** You have a dataset with hundreds of features, but many are redundant or correlated. PCA helps compress the data.\n",
    "\n",
    "**2. To Handle Multicollinearity (Highly Correlated Features)**\n",
    "\n",
    "*   If features are highly correlated, models like linear regression and logistic regression may struggle.\n",
    "*   PCA creates new uncorrelated features (principal components), which can improve model performance.\n",
    "*   **📌 Use case:** You have a dataset with many correlated variables (e.g., stock prices, sensor data). PCA helps remove redundancy.\n",
    "\n",
    "**3. To Speed Up Training for Computational Efficiency**\n",
    "\n",
    "*   Some machine learning models (e.g., SVMs, k-NN) slow down with too many features.\n",
    "*   Reducing dimensions with PCA can make training and predictions faster.\n",
    "*   **📌 Use case:** You're working with image data (e.g., 64x64 pixels = 4,096 features per image). Reducing dimensions with PCA speeds up training.\n",
    "\n",
    "**4. To Visualize High-Dimensional Data**\n",
    "\n",
    "*   PCA can reduce a dataset from many dimensions to 2D or 3D, making it easier to plot and visualize patterns.\n",
    "*   This is often used in exploratory data analysis (EDA).\n",
    "*   **📌 Use case:** You want to visualize customer clusters in a 100-feature dataset. PCA helps reduce it to 2D for plotting.\n",
    "\n",
    "**5. To Denoise Data (Feature Compression)**\n",
    "\n",
    "*   PCA helps filter out small variations or noise by focusing on the most important components.\n",
    "*   It can be useful for image compression or removing noise from sensor data.\n",
    "*   **📌 Use case:** You're working with speech recognition or ECG signals with lots of noise. PCA helps keep only useful information.\n",
    "\n",
    "**When Should You NOT Use PCA?**\n",
    "\n",
    "*   **❌ When feature interpretability is important** – PCA transforms features into abstract components that are harder to interpret.\n",
    "*   **❌ When you have categorical features** – PCA only works with numerical data.\n",
    "*   **❌ When your model already handles correlated features well** – Some algorithms (e.g., tree-based models like Random Forests) don't need PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Train Shape\n",
      "(120, 4)\n",
      "\n",
      "PCA\n",
      "[[ 1.83108424  0.01487935]\n",
      " [ 0.508027   -0.49990539]\n",
      " [-2.63234419  0.39631447]\n",
      " [ 1.2154392  -0.14342924]\n",
      " [ 2.08221549 -0.25701398]\n",
      " [-0.09269602 -0.73118799]\n",
      " [-2.26284668  0.89603188]\n",
      " [ 1.29859689 -0.44790598]\n",
      " [ 0.43577911 -0.11974613]\n",
      " [ 0.14836036 -0.41564899]\n",
      " [ 2.09364846  0.17637486]\n",
      " [-2.96574461 -0.11186675]\n",
      " [ 1.85392326  0.3710556 ]\n",
      " [-2.47613637  0.21406273]\n",
      " [-2.57299467  0.672843  ]\n",
      " [-0.60245412 -1.2666873 ]\n",
      " [ 1.87498856  0.00667557]\n",
      " [ 2.3507      0.26051941]\n",
      " [ 1.21267519 -0.78727627]\n",
      " [ 2.24033716 -0.28444214]\n",
      " [-0.03959956 -0.59214366]\n",
      " [ 3.00933971  0.63470325]\n",
      " [ 1.02468268  0.25993297]\n",
      " [-0.15467285 -0.71207189]\n",
      " [ 1.31036263 -0.23307084]\n",
      " [ 1.36575364 -0.17111743]\n",
      " [ 0.4318474  -1.20815433]\n",
      " [ 1.44608782 -0.40633872]\n",
      " [ 1.21713007  0.65883824]\n",
      " [ 1.69098714  0.04475602]\n",
      " [ 0.73640221  0.17474832]\n",
      " [-2.66564468 -0.1750786 ]\n",
      " [ 1.33330908 -0.60335789]\n",
      " [ 0.27456065 -0.5176113 ]\n",
      " [-0.25206012 -0.26112711]\n",
      " [ 0.08120361 -0.69243754]\n",
      " [ 0.81485523 -0.05456594]\n",
      " [ 2.54527845  0.51667517]\n",
      " [-2.60978421  0.60781276]\n",
      " [-3.30556737 -0.47331989]\n",
      " [ 1.7248434  -0.2498485 ]\n",
      " [ 0.25390875 -0.22441518]\n",
      " [-2.38085326  0.41646888]\n",
      " [-2.44023535  1.36364426]\n",
      " [ 1.39434947  0.47543812]\n",
      " [-2.8246223  -0.28680305]\n",
      " [ 1.31265718 -0.31016177]\n",
      " [-0.99213407 -0.75227064]\n",
      " [-2.89526809 -0.05722877]\n",
      " [ 0.8592616  -0.56553567]\n",
      " [ 2.85489749  0.73202858]\n",
      " [ 0.29712063 -0.30611301]\n",
      " [-2.86542406 -0.20317677]\n",
      " [ 2.54561122  0.29812148]\n",
      " [ 2.35919968  0.33427348]\n",
      " [ 2.27557253 -0.08218774]\n",
      " [ 2.0461184  -0.24855017]\n",
      " [-2.60507955  0.53265028]\n",
      " [-2.68698903  0.04462347]\n",
      " [ 1.59135904  0.20956009]\n",
      " [ 2.20631164  0.29437464]\n",
      " [-2.93919355 -0.90949724]\n",
      " [ 1.83124773  0.08053792]\n",
      " [-2.79253743 -0.14661034]\n",
      " [ 1.26144767 -0.80498396]\n",
      " [ 2.03792468  0.33297515]\n",
      " [-3.07611323 -0.30695746]\n",
      " [-2.79843659  0.35782404]\n",
      " [ 2.31651677  0.42380679]\n",
      " [-2.71843517  0.34115645]\n",
      " [-3.06188367 -0.45342551]\n",
      " [-2.34423299  0.76571138]\n",
      " [ 0.09543318 -0.83890558]\n",
      " [ 2.49320721  0.23257119]\n",
      " [ 3.32734866  0.49492051]\n",
      " [-2.75453226  0.34962026]\n",
      " [-2.71425368 -0.09171234]\n",
      " [-2.71062794  0.34141647]\n",
      " [ 0.56043101 -0.43435511]\n",
      " [ 0.82800587  0.30725731]\n",
      " [-2.94077452  0.10156   ]\n",
      " [-2.37581582  0.12275271]\n",
      " [ 0.64004595  0.13076264]\n",
      " [-2.47945614  0.43621331]\n",
      " [ 3.72098712  0.19507897]\n",
      " [-0.83849986 -1.0021094 ]\n",
      " [ 3.17390486  1.32101927]\n",
      " [ 0.85768063  0.44552157]\n",
      " [-2.71112998  0.84418242]\n",
      " [ 1.09325112 -0.19012346]\n",
      " [-2.70337646  1.20767409]\n",
      " [ 3.42766921  0.40361049]\n",
      " [-2.65438099  0.54252249]\n",
      " [-2.70954901 -0.16687482]\n",
      " [ 2.76881229  0.32699972]\n",
      " [-2.77709224  0.13812197]\n",
      " [ 2.07493143  0.10043231]\n",
      " [ 0.21883688 -0.36101101]\n",
      " [ 0.05584704 -0.32407894]\n",
      " [-0.38761603 -0.37207149]\n",
      " [ 1.83365784  0.08246638]\n",
      " [ 1.8728575   0.14942507]\n",
      " [ 1.21630092 -0.35414746]\n",
      " [ 1.48547789  0.23687998]\n",
      " [-2.58495082 -0.11825215]\n",
      " [-0.27616848 -0.68527206]\n",
      " [ 2.24343394  0.14085119]\n",
      " [ 1.33330908 -0.60335789]\n",
      " [-2.66201895  0.25805021]\n",
      " [ 0.96982063  0.20581504]\n",
      " [ 0.16781777 -0.2795732 ]\n",
      " [ 1.04223202 -0.10868565]\n",
      " [ 0.73134364 -0.39200777]\n",
      " [-2.69277263  0.62807724]\n",
      " [-2.61116911  0.46082466]\n",
      " [-2.75035077 -0.08324853]\n",
      " [ 1.8939228  -0.21495495]\n",
      " [ 0.15444993 -0.34382337]\n",
      " [ 3.42871556  1.11902311]\n",
      " [-2.91713562 -0.19523301]]\n",
      "\n",
      "PCA Shape\n",
      "(120, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)  # Reduce to 2 components\n",
    "\n",
    "print('X Train Shape')\n",
    "print(X_train.shape)\n",
    "\n",
    "X_pca = pca.fit_transform(X_train)\n",
    "\n",
    "print('\\nPCA')\n",
    "print(X_pca)\n",
    "\n",
    "print('\\nPCA Shape')\n",
    "print(X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas vs PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method                                      | When to Use                                                                                                | Pros                                                                                                  | Cons                                                                                                               |\n",
    "| :------------------------------------------ | :--------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------- |\n",
    "| Pandas Correlation Matrix (Feature Dropping) | When you have redundant features that don't add much value.                                              | - Keeps interpretability (e.g., in regression models).<br> - Simple to implement.                       | - May remove useful features.<br> - Manual threshold selection (e.g., >0.9 correlation).                       |\n",
    "| PCA (Principal Component Analysis)         | When you want to reduce dimensionality but keep all information.                                            | - Keeps all data (just transforms it).<br> - Helps avoid multicollinearity.                           | - Harder to interpret (new features are combinations of old ones).<br> - Might lose small but important details. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to Use What?**\n",
    "\n",
    "*   **For regression models (like Linear Regression, Logistic Regression)** → Use the correlation matrix to drop features (because collinearity causes unstable coefficients).\n",
    "*   **For machine learning models like SVM, Random Forest, or Neural Networks** → PCA can be useful (since models don't require interpretability, and PCA can improve efficiency).\n",
    "*   **If you need high interpretability** → Feature dropping is better.\n",
    "*   **If dimensionality is too high and you don't care about feature meaning** → Use PCA.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "*   If you can afford to lose features, drop them using pandas correlation matrix.\n",
    "*   If you want to keep all information, use PCA, but be aware that transformed features are no longer in the original space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the estimator's built in Regularization / C / Penalty / Alpha .. etc parameter to adjust the Regularization. \n",
    "\n",
    "- Lasso Regression (L1 penalty) can shrink some feature coefficients to zero, effectively removing redundant ones.\n",
    "- Ridge Regression (L2 penalty) penalizes large coefficients, reducing their impact but not removing them completely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name (Library & Syntax)                                                                     | Description                                                                                                                                                                                                                                                         | When to Use                                                                                                                                                           |\n",
    "| :---------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **1. Variance Threshold** (sklearn.feature_selection) <br> `VarianceThreshold(threshold=0.1)`  | Removes features with low variance.  It assumes that features with little variation across samples don't contribute much information to the model. It operates solely on the feature itself, not its relationship with the target variable.                               | - **High-dimensional data:** When you have many features, and you suspect many are constant or nearly constant. <br> - **Unsupervised learning:**  Useful as a first, simple filter since it doesn't require a target variable. <br> - **Preprocessing:** To remove obviously uninformative features before applying more complex methods.  |\n",
    "| **2. Mutual Information** (sklearn.feature_selection) <br> `mutual_info_classif(X, y)` (for classification) <br> `mutual_info_regression(X, y)` (for regression) | Measures the dependency between a feature and the target variable. Higher mutual information means a stronger relationship (linear or non-linear).  Captures non-linear relationships, unlike correlation.                                                                 | - **Non-linear relationships:** When you suspect features might have non-linear relationships with the target. <br> - **Classification & Regression:** Applicable to both. <br> - **Discrete target variables:** More naturally suited to classification (discrete targets), but can be used with regression by discretizing the target. |\n",
    "| **3. Correlation (df.corr())** (pandas) <br> `df.corr()`                                       | Calculates the Pearson correlation coefficient between each feature and the target variable (or between features themselves).  Measures *linear* relationships only.  Positive correlation means features increase/decrease together; negative means they vary inversely.   | - **Linear relationships:** When you expect a linear relationship between features and the target. <br> - **Regression:** Most directly applicable to regression problems. <br> - **Multicollinearity Detection:**  `df.corr()` applied to the features (not just the target) helps identify highly correlated features (multicollinearity), which can be problematic for some models (like linear regression).  You might remove one of a pair of highly correlated features. |\n",
    "| **4. Principal Component Analysis (PCA)** (sklearn.decomposition) <br> `PCA(n_components=k)` | A dimensionality reduction technique, *not strictly a feature selection method*, but often used in a similar way.  PCA transforms the original features into a new set of uncorrelated features (principal components) ordered by the amount of variance they explain. You can then select a subset of the top components. | - **Dimensionality reduction:** When you need to reduce the number of features while retaining as much variance as possible.  <br> - **Multicollinearity:**  PCA inherently handles multicollinearity by creating uncorrelated components. <br> - **Visualization:** Can be used to visualize high-dimensional data in 2D or 3D. <br> - **Noise reduction:** By keeping only the top components, you often filter out noise. <br> **Important:** PCA *transforms* features, it doesn't select the original ones. |\n",
    "| **5. Recursive Feature Elimination with Cross-Validation (RFECV)** (sklearn.feature_selection) <br> `RFECV(estimator, step=1, cv=5)` | Iteratively removes features and builds a model, using cross-validation to evaluate performance at each step. Selects the feature subset that yields the best cross-validation score.  Uses a model's internal feature importance (if available) or coefficients.     | - **Model-specific feature selection:** When you have a specific model in mind and want to find the best features *for that model*. <br> - **Automated process:** Automates the iterative feature elimination process. <br> - **Robustness:** Cross-validation makes the selection more robust to overfitting. <br> - **Computational cost:** Can be computationally expensive, especially with many features. |\n",
    "| **6. Regularization Methods** (sklearn.linear_model) <br> - **L1 (Lasso):** `Lasso(alpha=0.1)` <br> - **L2 (Ridge):** `Ridge(alpha=0.1)` <br> - **Elastic Net:** `ElasticNet(alpha=0.1, l1_ratio=0.5)` | These are *not standalone feature selection methods*, but rather techniques incorporated *within model training*.  They add a penalty term to the model's loss function that encourages smaller coefficients.  L1 regularization can drive coefficients to zero, effectively performing feature selection. | - **L1 (Lasso):**  Good for feature selection, as it tends to produce sparse models (many coefficients are zero).  <br> - **L2 (Ridge):**  Shrinks coefficients but doesn't typically set them to zero. Good for dealing with multicollinearity. <br> - **Elastic Net:** Combines L1 and L2 penalties; offers a balance between feature selection and coefficient shrinkage. <br> - **Linear models:** Most commonly used with linear regression, logistic regression, and other linear models (e.g., SVMs with linear kernels). <br> - **Overfitting prevention:**  Helps prevent overfitting, especially when you have many features. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models / Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few fits on a particular dataset gave me these best parameters:\n",
    "\n",
    "`HistGradientBoostingClassifier(l2_regularization=0.07, learning_rate=0.007,\n",
    "                               max_depth=6, max_iter=412)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pipe\n",
    "pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                         ('model', HistGradientBoostingClassifier(n_iter_no_change=10, random_state=rng))])\n",
    "\n",
    "#params\n",
    "max_iter = [375, 412, 430]\n",
    "learning_rate = np.linspace(0.005, 0.01, num=6).tolist()\n",
    "max_depth = [5, 6, 7]\n",
    "l2_regularization = [0.01, 0.03, 0.05, 0.07, 0.09]\n",
    "param_grid = {'model__learning_rate':learning_rate,\n",
    "               'model__max_iter':max_iter,\n",
    "               'model__max_depth': max_depth,\n",
    "               'model__l2_regularization': l2_regularization}\n",
    "\n",
    "grid = GridSearchCV(estimator=pipeline,param_grid=param_grid,cv=cv, scoring='f1', refit=True, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template for Pipeline. Edit column transformer variable to edit the data columns needed; edit 'model' step in pipeline to be whichever estimator needed\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "# make_column_transformer takes Tuples of the form (transformer, columns); columns take [] if transformer expects 2D array, no brackets if it expects 1D array\n",
    "column_transformer = make_column_transformer([\n",
    "    (StandardScaler(), ['column name']),\n",
    "    (OneHotEncoder(), ['column name'])],\n",
    "    n_jobs=-1)\n",
    "\n",
    "# name of step, estimator\n",
    "pipeline = Pipeline([\n",
    "    ('column transformer', column_transformer),\n",
    "    ('imputer',SimpleImputer()),\n",
    "    ('feature reduction', PCA()),\n",
    "    ('model', SGDClassifier())\n",
    "])\n",
    "\n",
    "# parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "\n",
    "}\n",
    "\n",
    "# split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=rng)\n",
    "\n",
    "grid = GridSearchCV(pipeline,\n",
    "                    param_grid=param_grid,\n",
    "                    n_jobs=-1,\n",
    "                    cv=5,\n",
    "                    return_train_score=True\n",
    "                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Never call fit() on test data\n",
    "- sklearn.preprocessing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
