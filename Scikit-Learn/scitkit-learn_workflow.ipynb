{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "Step by step how to handle data and create ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General Workflow Order in Scikit-Learn:**\n",
    "\n",
    "1.  **Data Preprocessing**\n",
    "\n",
    "    *   Handling missing values (e.g., `SimpleImputer`)\n",
    "    *   Encoding categorical variables (e.g., `OneHotEncoder` or `LabelEncoder`)\n",
    "    *   Scaling numerical features (e.g., `StandardScaler`, `MinMaxScaler`)\n",
    "    *   Handling outliers, transformations, etc.\n",
    "\n",
    "2.  **Feature Selection**\n",
    "\n",
    "    *   Finding correlation coefficients between each predictor pair with pandas's `df.corr()` (this tells us correlation between each predictor pair)\n",
    "    *   Finding p-values with `scipy.stats.pearsonr()`\n",
    "    *   Finding coefficients between each predictor/target pair using panadas's `df.corrwith()` (this tells us correlation between the each predictor/target pair)\n",
    "    *   Removing low-variance features (`VarianceThreshold`), if they do not have strong predictive power found with `df.corrwith()`\n",
    "    *   Selecting important features (`SelectKBest` filter method, Recursive Feature Elimination `RFCV` wrapper method)\n",
    "    *   Using model-based feature selection (e.g., `SelectFromModel` with Lasso or RandomForest) (eg, embedded methods)\n",
    "    \n",
    "3. **Train-Test Split** (`train_test_split`)\n",
    "\n",
    "4.  **Model Training & Evaluation**\n",
    "\n",
    "    *   Choosing a machine learning algorithm\n",
    "    *   Hyperparameter tuning\n",
    "    *   Performance evaluation (accuracy, precision, recall, F1-score, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper Methods vs Embedded Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature                     | Wrapper Methods                                         | Embedded Methods                                      |\n",
    "| :-------------------------- | :------------------------------------------------------ | :---------------------------------------------------- |\n",
    "| **Accuracy**                | High (can optimize feature set)                         | Moderate (depends on model regularization)             |\n",
    "| **Speed**                   | Slow (requires multiple model fits)                     | Fast (feature selection during training)                |\n",
    "| **Overfitting Risk**        | Higher                                                  | Lower (regularization helps)                           |\n",
    "| **Scalability**             | Poor for large datasets                                  | Works well on large datasets                           |\n",
    "| **Model Dependence**        | Works with any model                                     | Tied to a specific model                               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to Use What?**\n",
    "\n",
    "- Use Wrapper Methods when accuracy is critical and computational cost is not a concern (e.g., small datasets).\n",
    "- Use Embedded Methods when you need a faster, scalable, and regularized approach (e.g., large datasets).\n",
    "\n",
    "**Hybrid Approach?** You can combine both by using an embedded method (like Lasso) for initial feature filtering and then applying a wrapper method (like RFE) for fine-tuning. ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kfold vs Stratified KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, scikit learn chooses the right one for you automatically, but here is a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature         | KFold                               | StratifiedKFold                        |\n",
    "| :-------------- | :---------------------------------- | :-------------------------------------- |\n",
    "| Class Balance   | Not maintained                      | Maintained                              |\n",
    "| Use Case        | Regression, balanced classification | Imbalanced classification                 |\n",
    "| Distribution    | Splits randomly                     | Preserves class proportions             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Classifiers (for Classification Problems)**\n",
    "\n",
    "| Algorithm                                      | Pros                                                                                        | Cons                                                                 | Best For                                                    |\n",
    "| :--------------------------------------------- | :------------------------------------------------------------------------------------------ | :------------------------------------------------------------------- | :---------------------------------------------------------- |\n",
    "| Random Forest (RF)                             | Robust, handles missing data, avoids overfitting, interpretable (feature importance)           | Slower for large datasets                                             | General-purpose, tabular data                               |\n",
    "| Gradient Boosting (XGBoost, LightGBM, CatBoost) | High accuracy, handles non-linearity well                                                | Can overfit, requires tuning                                        | Structured/tabular data, small-to-medium datasets           |\n",
    "| Support Vector Machine (SVM)                   | Works well in high-dimensional space                                                      | Slow on large datasets                                               | Text classification, image recognition                     |\n",
    "| Logistic Regression                            | Simple, interpretable, works well for linear problems                                       | Poor performance for complex, non-linear data                     | Binary classification, medical studies                      |\n",
    "| Neural Networks (MLP, CNN, Transformers, etc.) | Powerful for complex data, scalable                                                          | Requires lots of data and tuning                                    | Deep learning tasks (images, NLP, time-series)               |\n",
    "| k-Nearest Neighbors (k-NN)                     | Simple, no training phase                                                                   | Slow for large datasets                                               | Small datasets, intuitive cases                              |\n",
    "| NaÃ¯ve Bayes                                    | Fast, works well with text/NLP                                                               | Assumes feature independence                                         | Spam detection, text classification                          |\n",
    "\n",
    "**Best Overall?**\n",
    "\n",
    "*   Random Forest or XGBoost for most structured/tabular data.\n",
    "*   Neural Networks (e.g., CNNs, Transformers) for deep learning (images, NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Regressors (for Regression Problems)**\n",
    "\n",
    "| Algorithm                                      | Pros                                                                    | Cons                                                 | Best For                                              |\n",
    "| :--------------------------------------------- | :----------------------------------------------------------------------- | :--------------------------------------------------- | :---------------------------------------------------- |\n",
    "| Linear Regression                              | Simple, interpretable                                                     | Poor fit for complex data                            | Basic regression, finance                            |\n",
    "| Ridge/Lasso Regression                         | Handles multicollinearity, reduces overfitting                           | Assumes linearity                                     | Feature selection, reducing overfitting                |\n",
    "| Random Forest Regressor                        | Handles non-linearity, robust                                            | Slower on large datasets                                 | General-purpose tabular data                           |\n",
    "| Gradient Boosting (XGBoost, LightGBM, CatBoost) | Highly accurate, works well for complex data                             | Sensitive to tuning                                  | Most regression tasks                                    |\n",
    "| Support Vector Regression (SVR)                | Works well in high-dimensional spaces                                    | Computationally expensive                             | High-dimensional data                                  |\n",
    "| Neural Networks (Deep Learning)                 | Captures complex relationships                                            | Needs lots of data                                   | Time series, large-scale problems, image-based predictions|\n",
    "| k-Nearest Neighbors (k-NN) Regression          | Simple, non-parametric                                                    | Slow for large datasets                                 | Small datasets                                          |\n",
    "\n",
    "**Best Overall?**\n",
    "\n",
    "*   XGBoost or Random Forest for structured data.\n",
    "*   Neural Networks for deep learning-based regression (e.g., stock prices, image-based predictions)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
